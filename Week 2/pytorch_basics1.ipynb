{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1855acb6-05bb-413f-b15e-26c04de445f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79d14ccd-987c-4af5-bd9e-75badd390ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tensors.\n",
    "x = torch.tensor(1., requires_grad=True)\n",
    "w = torch.tensor(2., requires_grad=True)\n",
    "b = torch.tensor(3., requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e7d699c-102e-4f96-8f10-b129507c5439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a computational graph.\n",
    "y = w * x + b    # y = 2 * x + 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e612c24-d84c-4951-a8cb-721cfd27161b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# Compute gradients.\n",
    "y.backward()\n",
    "\n",
    "# Print out the gradients.\n",
    "print(x.grad)    # x.grad = 2 \n",
    "print(w.grad)    # w.grad = 1 \n",
    "print(b.grad)    # b.grad = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e511571-0e9a-4950-aa5a-84356167e4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tensors of shape (10, 3) and (10, 2).\n",
    "x = torch.randn(10, 3)\n",
    "y = torch.randn(10, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d99afe5c-e0c0-46b6-a002-f3ab6ab6ee16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w:  Parameter containing:\n",
      "tensor([[ 0.1954,  0.1499, -0.2221],\n",
      "        [ 0.0324,  0.4372, -0.2863]], requires_grad=True)\n",
      "b:  Parameter containing:\n",
      "tensor([-0.4896,  0.5448], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Build a fully connected layer.\n",
    "linear = nn.Linear(3, 2)\n",
    "print ('w: ', linear.weight)\n",
    "print ('b: ', linear.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3066867a-6d31-40a0-b9c2-4a63ecbb3a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build loss function and optimizer.\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(linear.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "186c6d76-35eb-48bd-8a96-ccb96807927f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass.\n",
    "pred = linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4c07ffa-867f-444a-aedf-a0aff81c2aae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  0.766990065574646\n"
     ]
    }
   ],
   "source": [
    "# Compute loss.\n",
    "loss = criterion(pred, y)\n",
    "print('loss: ', loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80efabc2-81ee-4dab-b78f-3784e16c2ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward pass.\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d07ad5d-9a66-494d-9835-8bdefdfe3c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dL/dw:  tensor([[-0.1221, -0.0342, -0.3245],\n",
      "        [ 0.4167,  0.1309,  0.0797]])\n",
      "dL/db:  tensor([0.0244, 0.5076])\n"
     ]
    }
   ],
   "source": [
    "# Print out the gradients.\n",
    "print ('dL/dw: ', linear.weight.grad) \n",
    "print ('dL/db: ', linear.bias.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d2cfa4b3-eaa8-4cd9-aab3-5fcf8e224136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-step gradient descent.\n",
    "optimizer.step()\n",
    "\n",
    "# You can also perform gradient descent at the low level.\n",
    "# linear.weight.data.sub_(0.01 * linear.weight.grad.data)\n",
    "# linear.bias.data.sub_(0.01 * linear.bias.grad.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91e2a87f-2d5f-4461-8439-4828ac0eaa5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss after 1 step optimization:  0.7612648606300354\n"
     ]
    }
   ],
   "source": [
    "# Print out the loss after 1-step gradient descent.\n",
    "pred = linear(x)\n",
    "loss = criterion(pred, y)\n",
    "print('loss after 1 step optimization: ', loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9df0605a-a180-4196-a3df-029e5ea37eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a numpy array.\n",
    "x = np.array([[1, 2], [3, 4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b127239-b370-48aa-9ec4-54ca90f0471c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the numpy array to a torch tensor.\n",
    "y = torch.from_numpy(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d7f8f132-9111-497e-ab48-9fa512b1f465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the torch tensor to a numpy array.\n",
    "z = y.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "513f3fae-9fc2-4acc-82bc-b5fb02c13f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Download and construct CIFAR-10 dataset.\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='../../data/',\n",
    "                                             train=True, \n",
    "                                             transform=transforms.ToTensor(),\n",
    "                                             download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "01f59218-41c8-479a-844d-9f4943ae5d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 32, 32])\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "# Fetch one data pair (read data from disk).\n",
    "image, label = train_dataset[0]\n",
    "print (image.size())\n",
    "print (label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6d44c395-9a2e-4a70-8a62-5901d9b8c8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loader (this provides queues and threads in a very simple way).\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=64, \n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e180c796-b744-4243-b41c-f19b669e869c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When iteration starts, queue and thread start to load data from files.\n",
    "data_iter = iter(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5433e1af-03cb-4582-a154-242e6aced0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mini-batch images and labels.\n",
    "images, labels = next(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ca64fc8e-97ef-46fe-bffa-a872013d0daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual usage of the data loader is as below.\n",
    "for images, labels in train_loader:\n",
    "    # Training code should be written here.\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1bf594b5-c1ab-4730-a349-0c9824d3e911",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self):\n",
    "        # Initialize your dataset (e.g., a list of file paths or data)\n",
    "        self.data = [...]  # Add the actual data or file paths here\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Example: Replace this with the actual logic to load and preprocess data\n",
    "        # e.g., using numpy, PIL.Image, or other data processing tools\n",
    "        image = np.load(self.data[index])  # Replace with actual data loading\n",
    "        label = ...  # Replace with the corresponding label logic\n",
    "        image = torch.tensor(image, dtype=torch.float32)  # Convert to tensor\n",
    "        return image, label\n",
    "\n",
    "    def __len__(self):\n",
    "        # This should return the total number of samples in the dataset\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "17273e6b-87b6-4f68-974a-7891cdd1cf83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can then use the prebuilt data loader. \n",
    "custom_dataset = CustomDataset()\n",
    "train_loader = torch.utils.data.DataLoader(dataset=custom_dataset,\n",
    "                                           batch_size=64, \n",
    "                                           shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c5b00e66-a193-4291-87bb-888351a9722a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tharacaba/opt/anaconda3/envs/astro/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/tharacaba/opt/anaconda3/envs/astro/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Download and load the pretrained ResNet-18.\n",
    "resnet = torchvision.models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d0886077-6fd1-4aa8-b0a6-ab0ac6fa7583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to finetune only the top layer of the model, set as below.\n",
    "for param in resnet.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "14c61c09-a546-4672-9eae-a19fafcdb9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the top layer for finetuning.\n",
    "resnet.fc = nn.Linear(resnet.fc.in_features, 100)  # 100 is an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1f7d4948-84ad-4bcd-b2ec-ed4690e343ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 100])\n"
     ]
    }
   ],
   "source": [
    "# Forward pass.\n",
    "images = torch.randn(64, 3, 224, 224)\n",
    "outputs = resnet(images)\n",
    "print (outputs.size())     # (64, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "47e94e63-a19d-4fd2-80af-d17f78ea960e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and load the entire model.\n",
    "torch.save(resnet, 'model.ckpt')\n",
    "model = torch.load('model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7a2166ed-7303-4776-b886-7666c89dccad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save and load only the model parameters (recommended).\n",
    "torch.save(resnet.state_dict(), 'params.ckpt')\n",
    "resnet.load_state_dict(torch.load('params.ckpt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ee65de-d0dc-49b1-b672-80588fbe8ff3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
